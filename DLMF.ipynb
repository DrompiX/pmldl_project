{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLMF.ipynb",
      "provenance": [],
      "mount_file_id": "17eM9PJMXdI2B0lGBm3rk8flMImKoIi7Z",
      "authorship_tag": "ABX9TyO9C15iu8XeLiiESyoAQpFE"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyv1rRckc3fq",
        "colab_type": "text"
      },
      "source": [
        "Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gea07DAN68EM",
        "colab_type": "code",
        "outputId": "f42c3942-6cd2-47ad-cdc6-31c5212945c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import scipy.sparse as sp\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import heapq\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import easydict\n",
        "from argparse import ArgumentParser\n",
        "from time import time\n",
        "\n",
        "from keras import backend as K\n",
        "from keras import optimizers\n",
        "from keras.layers import Input, Dense, Lambda\n",
        "from keras.models import Model"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9LU3tMOdGLY",
        "colab_type": "text"
      },
      "source": [
        "Downloading the small dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJS1aA8FYNby",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "658af93f-5e3b-404f-8cbf-ca5a2dfc2d1d"
      },
      "source": [
        "!wget \"https://drive.google.com/uc?export=download&id=1bEDAdHZFH3_UukpEpx9WC1P0NJDVRSI8\" -O ratings.dat"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-12 06:28:07--  https://drive.google.com/uc?export=download&id=1bEDAdHZFH3_UukpEpx9WC1P0NJDVRSI8\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.14.78, 2607:f8b0:4007:80b::200e\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.14.78|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-14-08-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/f92uro2cv4mmbnhbujc119pq0osomi6m/1589264850000/08871784378030230384/*/1bEDAdHZFH3_UukpEpx9WC1P0NJDVRSI8?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-05-12 06:28:08--  https://doc-14-08-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/f92uro2cv4mmbnhbujc119pq0osomi6m/1589264850000/08871784378030230384/*/1bEDAdHZFH3_UukpEpx9WC1P0NJDVRSI8?e=download\n",
            "Resolving doc-14-08-docs.googleusercontent.com (doc-14-08-docs.googleusercontent.com)... 172.217.5.193, 2607:f8b0:4007:80d::2001\n",
            "Connecting to doc-14-08-docs.googleusercontent.com (doc-14-08-docs.googleusercontent.com)|172.217.5.193|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/octet-stream]\n",
            "Saving to: ‘ratings.dat’\n",
            "\n",
            "ratings.dat             [  <=>               ]   2.52M  12.2MB/s    in 0.2s    \n",
            "\n",
            "2020-05-12 06:28:09 (12.2 MB/s) - ‘ratings.dat’ saved [2644288]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ipNUQTcj1bz",
        "colab_type": "text"
      },
      "source": [
        "Change arguments here "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sJbzGNN_3yV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    args = easydict.EasyDict({\n",
        "    \"dataset\": \"2k\",   \n",
        "    \"user_layers\": \"[264,64]\",\n",
        "    \"item_layers\": \"[512,64]\",\n",
        "    \"epochs\": 1,\n",
        "    \"lr\": 0.001,\n",
        "    \"batch_size\": 256,\n",
        "    \"num_neg\": 7,\n",
        "    \"topN\": 10\n",
        "})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydTiitprkC2P",
        "colab_type": "text"
      },
      "source": [
        "Reading the file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVjQ1tHHdXpz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataSet(object):\n",
        "\n",
        "    def __init__(self): \n",
        "        filename = 'ratings.dat'\n",
        "        data_separator = '::'\n",
        "\n",
        "        self.data_list, self.num_users, self.num_items, self.max_rate = \\\n",
        "            self.load_rating_file_as_list(filename, separator=data_separator)\n",
        "        self.train, self.test = self.get_train_test()\n",
        "        self.data_matrix = self.get_data_matrix()\n",
        "        # self.train_data = self.get_train_instances()\n",
        "        self.test_ratings, self.test_negatives = self.get_test_instances()\n",
        "\n",
        "    @staticmethod\n",
        "    def load_rating_file_as_list(filename, separator='\\t'):\n",
        "        print('loading rating file: %s...' % filename)\n",
        "        data = []\n",
        "        num_users, num_items, max_rate = 0, 0, 0\n",
        "        with open(filename, 'r') as file:\n",
        "            for line in file:\n",
        "                if line is not None and line != '':\n",
        "                    arr = line.strip().split(separator)\n",
        "                    u, i, rating, timestamp = int(arr[0]), int(arr[1]), float(arr[2]), int(arr[3])\n",
        "                    data.append([u, i, rating, timestamp])\n",
        "                    if u > num_users:\n",
        "                        num_users = u\n",
        "                    if i > num_items:\n",
        "                        num_items = i\n",
        "                    if rating > max_rate:\n",
        "                        max_rate = rating\n",
        "        print('number of users: %d, number of items: %d' % (num_users, num_items))\n",
        "        return data, num_users, num_items, max_rate\n",
        "\n",
        "    def get_data_matrix(self):\n",
        "        mat = np.zeros((self.num_users, self.num_items))\n",
        "        for line in self.train:\n",
        "            user, item, rating = line[0], line[1], line[2]\n",
        "            mat[user, item] = rating\n",
        "        return mat\n",
        "\n",
        "    def get_train_test(self):\n",
        "        print('splitting train and test data...')\n",
        "        data = self.data_list\n",
        "        data = sorted(data, key=lambda x: (x[0], x[3]))\n",
        "\n",
        "        train = []\n",
        "        test = []\n",
        "        for i in range(len(data) - 1):\n",
        "            user, item, rating = data[i][0], data[i][1], data[i][2]\n",
        "            if data[i][0] != data[i + 1][0]:\n",
        "                test.append((user - 1, item - 1, rating))\n",
        "            else:\n",
        "                train.append((user - 1, item - 1, rating))\n",
        "\n",
        "        test.append((data[-1][0] - 1, data[-1][1] - 1, data[-1][2]))\n",
        "        return train, test\n",
        "\n",
        "    def get_train_instances(self, num_negatives):\n",
        "        print('getting train instances...')\n",
        "        user_input = []\n",
        "        item_input = []\n",
        "        ratings = []\n",
        "        for i in self.train:\n",
        "            u = i[0]\n",
        "            user_input.append(u)\n",
        "            item_input.append(i[1])\n",
        "            ratings.append(i[2])\n",
        "\n",
        "            # negative samples\n",
        "            item_list = []\n",
        "            for t in range(num_negatives):\n",
        "                while True:\n",
        "                    j = np.random.randint(self.num_items)\n",
        "                    if self.data_matrix[u, j] == 0 and j not in item_list:\n",
        "                        user_input.append(u)\n",
        "                        item_input.append(j)\n",
        "                        ratings.append(0)\n",
        "\n",
        "                        item_list.append(j)\n",
        "                        break\n",
        "        return user_input, item_input, ratings\n",
        "\n",
        "    def get_test_instances(self, num_negatives=100):\n",
        "        print('getting test instances...')\n",
        "        np.random.seed(34)\n",
        "        test_ratings = []\n",
        "        test_negatives = []\n",
        "        for i in self.test:\n",
        "            u = i[0]\n",
        "            test_ratings.append([u, i[1], i[2]])\n",
        "            # negative samples\n",
        "            negative = []\n",
        "            for t in range(num_negatives):\n",
        "                while True:\n",
        "                    j = np.random.randint(self.num_items)\n",
        "                    if self.data_matrix[u, j] == 0 and j not in negative:\n",
        "                        negative.append(j)\n",
        "                        break\n",
        "            test_negatives.append(negative)\n",
        "        return test_ratings, test_negatives\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zGlEyD3kJON",
        "colab_type": "text"
      },
      "source": [
        "Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxK33hshhdM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_model = None\n",
        "_test_ratings = None\n",
        "_test_negatives = None\n",
        "_data_matrix = None\n",
        "\n",
        "\n",
        "def evaluate_model(model, test_ratings, test_negatives, data_matrix, k):\n",
        "    global _model\n",
        "    global _test_ratings\n",
        "    global _test_negatives\n",
        "    global _data_matrix\n",
        "\n",
        "    _model = model\n",
        "    _test_ratings = test_ratings\n",
        "    _test_negatives = test_negatives\n",
        "    _data_matrix = data_matrix\n",
        "\n",
        "    hits, ndcgs = [], []\n",
        "    for i in range(len(_test_ratings)):\n",
        "        (hr, ndcg) = _evaluate_one_rating(i, k=k)\n",
        "        hits.append(hr)\n",
        "        ndcgs.append(ndcg)\n",
        "    return hits, ndcgs\n",
        "\n",
        "\n",
        "def _evaluate_one_rating(idx, k):\n",
        "    rating = _test_ratings[idx]\n",
        "    items = _test_negatives[idx]\n",
        "    user = rating[0]\n",
        "    gt_item = rating[1]\n",
        "    items.append(gt_item)\n",
        "\n",
        "    items_input = []\n",
        "    users_input = []\n",
        "    for item in items:\n",
        "        items_input.append(_data_matrix[:, item])\n",
        "        users_input.append(_data_matrix[user])\n",
        "    predictions = _model.predict([np.array(users_input), np.array(items_input)],\n",
        "                                 batch_size=100 + 1,\n",
        "                                 verbose=0)\n",
        "\n",
        "    map_item_score = {}\n",
        "    for idx, item in enumerate(items):\n",
        "        map_item_score[item] = predictions[idx]\n",
        "\n",
        "    items.pop()\n",
        "    rank_list = heapq.nlargest(k, map_item_score, key=map_item_score.get)\n",
        "    hr = get_hit_ratio(rank_list, gt_item)\n",
        "    ndcg = get_ndcg(rank_list, gt_item)\n",
        "    return hr, ndcg\n",
        "\n",
        "\n",
        "def get_hit_ratio(rank_list, gt_item):\n",
        "    if gt_item in rank_list:\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "\n",
        "def get_ndcg(rank_list, gt_item):\n",
        "    for idx, item in enumerate(rank_list):\n",
        "        if item == gt_item:\n",
        "            return math.log(2) / math.log(idx + 2)\n",
        "    return 0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euirKT8EkH6T",
        "colab_type": "text"
      },
      "source": [
        "The model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUEX6hA_cVH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DMF(object):\n",
        "    def __init__(self,\n",
        "                 num_users,\n",
        "                 num_items,\n",
        "                 user_layers,\n",
        "                 item_layers,\n",
        "                 lr):\n",
        "        self.max_rate = dataset.max_rate\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.user_layers = user_layers\n",
        "        self.item_layers = item_layers\n",
        "        self.lr = lr\n",
        "\n",
        "    @staticmethod\n",
        "    def init_normal(shape, dtype=None):\n",
        "        return K.random_normal(shape=shape, stddev=0.01, dtype=dtype)\n",
        "\n",
        "    def mse(self, y_true, y_pred):\n",
        "        loss = (y_true - y_pred)**2\n",
        "        return K.mean(loss)\n",
        "\n",
        "    def cosine_similarity_relu(self, inputs):\n",
        "        x, y = inputs[0], inputs[1]\n",
        "        vec = K.batch_dot(x, y) / (K.sqrt(K.batch_dot(x, x) * K.batch_dot(y, y)))\n",
        "        return K.maximum(vec, 1.0e-6)\n",
        "\n",
        "    def get_model(self):\n",
        "        user_input = Input(shape=(self.num_items,), dtype='float32', name='user_input')\n",
        "        item_input = Input(shape=(self.num_users,), dtype='float32', name='item_input')\n",
        "\n",
        "        user_vector = None\n",
        "        item_vector = None\n",
        "        for i in range(len(self.user_layers)):\n",
        "            layer = Dense(self.user_layers[i],\n",
        "                          activation='relu',\n",
        "                          kernel_initializer=self.init_normal,\n",
        "                          bias_initializer=self.init_normal,\n",
        "                          name='user_layer%d' % (i + 1))\n",
        "            if i == 0:\n",
        "                user_vector = layer(user_input)\n",
        "            else:\n",
        "                user_vector = layer(user_vector)\n",
        "\n",
        "        for i in range(len(self.item_layers)):\n",
        "            layer = Dense(self.item_layers[i],\n",
        "                          activation='relu',\n",
        "                          kernel_initializer=self.init_normal,\n",
        "                          bias_initializer=self.init_normal,\n",
        "                          name='item_layer%d' % (i + 1))\n",
        "            if i == 0:\n",
        "                item_vector = layer(item_input)\n",
        "            else:\n",
        "                item_vector = layer(item_vector)\n",
        "\n",
        "        y_predict = Lambda(function=self.cosine_similarity_relu, name='predict')([user_vector, item_vector])\n",
        "        model = Model(inputs=[user_input, item_input], outputs=y_predict)\n",
        "        model.compile(optimizer=optimizers.Adam(lr=self.lr), loss=self.mse)\n",
        "        return model\n",
        "\n",
        "\n",
        "def generate_user_item_input(users, items, ratings, data_matrix, batch_size):\n",
        "    batch = math.ceil(len(items) / batch_size)\n",
        "    for batch_id in range(batch):\n",
        "        user_input, item_input = [], []\n",
        "        max_idx = min(len(items), (batch_id + 1) * batch_size)\n",
        "        for idx in range(batch_id * batch_size, max_idx):\n",
        "            u = users[idx]\n",
        "            i = items[idx]\n",
        "            item_input.append(data_matrix[:, i])\n",
        "            user_input.append(data_matrix[u])\n",
        "        target_ratings = ratings[batch_id * batch_size:max_idx]\n",
        "        yield [np.array(user_input), np.array(item_input)], target_ratings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP_ioKELkSO5",
        "colab_type": "text"
      },
      "source": [
        "Training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjqMCuG1TMMm",
        "colab_type": "code",
        "outputId": "4877d1f0-a860-46f9-d225-4068031cd5f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "epochs = args.epochs\n",
        "dmf_user_layers = eval(args.user_layers)\n",
        "dmf_item_layers = eval(args.item_layers)\n",
        "batch_size = args.batch_size\n",
        "data_set = args.dataset\n",
        "lr = args.lr\n",
        "topN = args.topN\n",
        "num_train_negatives = args.num_neg\n",
        "\n",
        "print(args)\n",
        "\n",
        "\n",
        "if not os.path.exists('model'):\n",
        "    os.mkdir('model')\n",
        "model_out_file = 'model/%s_u%s_i%s_%d_%d.h5' % (data_set, str(dmf_user_layers),\n",
        "                                                str(dmf_item_layers), batch_size, time())\n",
        "\n",
        "# load data set\n",
        "dataset = DataSet()\n",
        "\n",
        "# initialize DMF\n",
        "dmf = DMF(num_users=dataset.num_users,\n",
        "          num_items=dataset.num_items,\n",
        "          user_layers=dmf_user_layers,\n",
        "          item_layers=dmf_item_layers,\n",
        "          lr=lr)\n",
        "model = dmf.get_model()\n",
        "model.summary()\n",
        "\n",
        "(hits, ndcgs) = evaluate_model(model, dataset.test_ratings, dataset.test_negatives, dataset.data_matrix, topN)\n",
        "hr, ndcg, loss = np.array(hits).mean(), np.array(ndcgs).mean(), -1\n",
        "best_hr, best_ndcg = hr, ndcg\n",
        "best_iter = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    start = time()\n",
        "\n",
        "    # Generate training instances\n",
        "    users, items, ratings = dataset.get_train_instances(num_train_negatives)\n",
        "\n",
        "    print('start training...')\n",
        "\n",
        "    history = model.fit_generator(generate_user_item_input(users, items, ratings, dataset.data_matrix, batch_size),\n",
        "                                  steps_per_epoch=math.ceil(len(users) / batch_size),\n",
        "                                  epochs=1)\n",
        "\n",
        "    end = time()\n",
        "    print('Epoch %d Finished. [%.1f s]' % (epoch + 1, end - start))\n",
        "    (hits, ndcgs) = evaluate_model(model, dataset.test_ratings, dataset.test_negatives, dataset.data_matrix, topN)\n",
        "    hr, ndcg, loss = np.array(hits).mean(), np.array(ndcgs).mean(),  history.history['loss'][0]\n",
        "\n",
        "    # print(\"MSE = %.2f\" % (mse(np.array(ratings), model.predict(generate_user_item_input(users, items, ratings, dataset.data_matrix, batch_size)))))\n",
        "    print('MSE = %.4f'\n",
        "          % (ndcg))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'dataset': '2k', 'user_layers': '[264,64]', 'item_layers': '[512,64]', 'epochs': 1, 'lr': 0.001, 'batch_size': 256, 'num_neg': 7, 'topN': 10}\n",
            "loading rating file: ratings.dat...\n",
            "number of users: 342, number of items: 38897\n",
            "splitting train and test data...\n",
            "getting test instances...\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "user_input (InputLayer)         (None, 38897)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "item_input (InputLayer)         (None, 342)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "user_layer1 (Dense)             (None, 264)          10269072    user_input[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "item_layer1 (Dense)             (None, 512)          175616      item_input[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "user_layer2 (Dense)             (None, 64)           16960       user_layer1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "item_layer2 (Dense)             (None, 64)           32832       item_layer1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "predict (Lambda)                (None, 1)            0           user_layer2[0][0]                \n",
            "                                                                 item_layer2[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 10,494,480\n",
            "Trainable params: 10,494,480\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "getting train instances...\n",
            "start training...\n",
            "Epoch 1/1\n",
            "3460/3460 [==============================] - 171s 49ms/step - loss: 0.7957\n",
            "Epoch 1 Finished. [174.8 s]\n",
            "MSE = 0.5829\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxlC5RJKMyPj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}